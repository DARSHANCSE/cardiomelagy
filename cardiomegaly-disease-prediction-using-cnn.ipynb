{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2999507,"sourceType":"datasetVersion","datasetId":1837618},{"sourceId":9525700,"sourceType":"datasetVersion","datasetId":5800509}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About Dataset and modeling","metadata":{}},{"cell_type":"markdown","source":"**In this dataset, our goal is to develop a model using CNN that can diagnose Cardiomegaly using real X-ray images taken from the chests of different individuals. To accomplish this, a large number of these images have been collected and classified into two categories:\\\nindividuals who have the disease and those who do not. Our CNN model is intended to predict whether an individual is diseased or not based on their image.\\\nComplete dataset information is available at the following link:\\\nhttps://www.kaggle.com/datasets/rahimanshu/cardiomegaly-disease-prediction-using-cnn**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, BatchNormalization , Flatten ,Dense , AveragePooling2D\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping , ModelCheckpoint\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split\nimport os\nimport glob\n\nimport numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sn","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:06.089003Z","iopub.execute_input":"2024-10-01T17:14:06.090160Z","iopub.status.idle":"2024-10-01T17:14:06.100850Z","shell.execute_reply.started":"2024-10-01T17:14:06.090112Z","shell.execute_reply":"2024-10-01T17:14:06.099411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing the Data","metadata":{}},{"cell_type":"markdown","source":"*In our original dataset, we only have two sections: train and test, which are insufficient. For a CNN model, we need a structure of **`Train+Validation+Test.`** Therefore, we must divide our train data into two parts and create a validation set.*","metadata":{}},{"cell_type":"code","source":"# In this code, we read the link of each photo and its label and save them in to seprated lists.\n\ndef load_data_train_part(image_dir, test_size=0.2 ):\n    data = []\n    labels = []\n    class_dirs = [d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))]\n    \n    for class_dir in class_dirs:\n        class_label = class_dir\n        image_files = glob.glob(os.path.join(image_dir, class_dir, \"*.png\"))  # adjust as necessary\n        data.extend(image_files)\n        labels.extend([class_label]*len(image_files))\n\n\n    # Further split the train data into train and validation sets\n    train_files, val_files, train_labels, val_labels = train_test_split(data, labels, \n                                                                        test_size=test_size, \n                                                                        stratify=labels)\n    return train_files, train_labels, val_files, val_labels","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:06.103123Z","iopub.execute_input":"2024-10-01T17:14:06.103953Z","iopub.status.idle":"2024-10-01T17:14:06.120008Z","shell.execute_reply.started":"2024-10-01T17:14:06.103914Z","shell.execute_reply":"2024-10-01T17:14:06.119020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_files, train_labels, val_files, val_labels = \\\nload_data_train_part(image_dir = r'/kaggle/input/dataset/train/train')","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:06.121379Z","iopub.execute_input":"2024-10-01T17:14:06.121795Z","iopub.status.idle":"2024-10-01T17:14:06.158353Z","shell.execute_reply.started":"2024-10-01T17:14:06.121757Z","shell.execute_reply":"2024-10-01T17:14:06.157464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we import the test data\n\ndef load_data_test_part(image_dir):\n    data = []\n    labels = []\n    class_dirs = [d for d in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, d))]\n    \n    for class_dir in class_dirs:\n        class_label = class_dir\n        image_files = glob.glob(os.path.join(image_dir, class_dir, \"*.png\"))  # adjust as necessary\n        data.extend(image_files)\n        labels.extend([class_label]*len(image_files))\n\n    # Create a DataFrame with the image paths and labels\n    df_test_part = pd.DataFrame({\n        'filename': data,\n        'class': labels\n    })\n    \n    test_files = list( df_test_part.iloc[ : , 0] ) # return first column of the df_test_part as a list (file names)\n    test_labels =list( df_test_part.iloc[ : , 1] ) # return second column of the df_test_part as a list(class labels)\n    \n    return test_files , test_labels","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:06.161182Z","iopub.execute_input":"2024-10-01T17:14:06.161706Z","iopub.status.idle":"2024-10-01T17:14:06.169849Z","shell.execute_reply.started":"2024-10-01T17:14:06.161666Z","shell.execute_reply":"2024-10-01T17:14:06.168814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_files, test_labels = \\\nload_data_test_part(image_dir= '/kaggle/input/cardiomegaly-disease-prediction-using-cnn/test/test')","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:06.171233Z","iopub.execute_input":"2024-10-01T17:14:06.171569Z","iopub.status.idle":"2024-10-01T17:14:06.192205Z","shell.execute_reply.started":"2024-10-01T17:14:06.171542Z","shell.execute_reply":"2024-10-01T17:14:06.191376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert labels to one-hot encodings","metadata":{}},{"cell_type":"code","source":"# Convert labels to one-hot encodings\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(train_labels)  # Assuming train_labels is your list of training labels\n\n# One-hot encode your labels\ntrain_labels_enc = to_categorical(label_encoder.transform(train_labels))\nval_labels_enc = to_categorical(label_encoder.transform(val_labels))\ntest_labels_enc = to_categorical(label_encoder.transform(test_labels))","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:06.193573Z","iopub.execute_input":"2024-10-01T17:14:06.194007Z","iopub.status.idle":"2024-10-01T17:14:06.209685Z","shell.execute_reply.started":"2024-10-01T17:14:06.193953Z","shell.execute_reply":"2024-10-01T17:14:06.208554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the class names and corresponding integer encodings\nclass_names = label_encoder.classes_\nclass_numbers = label_encoder.transform(label_encoder.classes_)\n\n# Print class names with the assigned numbers\nclass_dict = dict(zip(class_names, class_numbers))\nprint(class_dict)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:06.211048Z","iopub.execute_input":"2024-10-01T17:14:06.211397Z","iopub.status.idle":"2024-10-01T17:14:06.219758Z","shell.execute_reply.started":"2024-10-01T17:14:06.211365Z","shell.execute_reply":"2024-10-01T17:14:06.218682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Provide image preview","metadata":{}},{"cell_type":"code","source":"#This function selects and displays 5 photos randomly from the address of the files. \n#The link of these photos should be inside a list.\n\ndef show_sample_images(image_files, labels, num_samples=5):\n    if len(image_files) < num_samples:\n        print(\"Not enough images to show\")\n        return\n\n    sample_indices = random.sample(range(len(image_files)), num_samples)\n    sample_files = [image_files[i] for i in sample_indices]\n    sample_labels = [labels[i] for i in sample_indices]\n\n    fig, axes = plt.subplots(1, num_samples, figsize=(20, 10))\n    for i, (file, label) in enumerate(zip(sample_files, sample_labels)):\n        img = mpimg.imread(file)\n        axes[i].imshow(img, cmap='gray')  # For grayscale images, use cmap='gray' and for rbg pictures use None.\n        axes[i].axis('off')\n        axes[i].set_title(f\"{label}\\n{os.path.basename(file)}\", fontsize=18)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:06.222895Z","iopub.execute_input":"2024-10-01T17:14:06.223280Z","iopub.status.idle":"2024-10-01T17:14:06.232711Z","shell.execute_reply.started":"2024-10-01T17:14:06.223253Z","shell.execute_reply":"2024-10-01T17:14:06.231827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I use train files and labels\nshow_sample_images(train_files,train_labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:06.233861Z","iopub.execute_input":"2024-10-01T17:14:06.234189Z","iopub.status.idle":"2024-10-01T17:14:07.142991Z","shell.execute_reply.started":"2024-10-01T17:14:06.234162Z","shell.execute_reply":"2024-10-01T17:14:07.141613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Normalization","metadata":{}},{"cell_type":"markdown","source":"In computer vision, the pixel normalization technique is often used to speed up model learning. The normalization of an image consists in dividing each of its pixel values by the maximum value that a pixel can take (255 for an 8-bit image, 4095 for a 12-bit image, 65 535 for a 16-bit image)[1].\n\n**The general formula for normalization is to divide by the maximum possible value for that bit depth:**\n\n**Normalized Value = 1 / [ 2^(bit depth) - 1 ]**\n\nHere are the normalization factors for different bit depths based on the formula provided:\n\n- For 8-bit images:  1 / (2^8)-1 = 1/255\n- For 12-bit images: 1 / (2^12)-1 = 1/4095\n- For 16-bit images: 1 / (2^16)-1 = 1/65535\n\nThese factors are used to scale the pixel values of images from their original range to a range of [0, 1]. For an 8-bit image, you divide each pixel value by 255, for a 12-bit image by 4095, and for a 16-bit image by 65535 to achieve normalization.\n\nLink[1]\nhttps://www.imaios.com/en/resources/blog/ct-images-normalization-zero-centering-and-standardization#:~:text=,bit%20image","metadata":{}},{"cell_type":"markdown","source":"# Zero-centering","metadata":{}},{"cell_type":"markdown","source":"Zero-centering is a common preprocessing step in deep learning where you adjust the data to have a mean of zero. This process involves subtracting the mean value of the pixel intensity over the entire training set from each pixel value. By doing this, the data distribution is centered around zero, which often improves the training efficiency and stability of the model.\n\n\nZero-centering can be beneficial for various activation functions. Here's a simplified overview:\n\n1. **Sigmoid and Tanh**: Zero-centering is almost necessary because these functions are symmetric around the origin. Without zero-centering, if all the inputs are positive (as they would be for image data without zero-centering), the gradients can be consistently positive or negative, leading to inefficient learning dynamics.\n\n2. **ReLU and its variants (e.g., Leaky ReLU, Parametric ReLU)**: Zero-centering is less critical because these functions are not symmetric, and they can handle non-zero-centered input better. However, it can still potentially improve training by providing a balanced distribution of positive and negative values.\n\n3. **Linear or Identity**: Zero-centering is optional because these functions are not affected by the mean of the data, but it can still help with the overall conditioning of the optimization problem.\n\nHere's a table summarizing this information:\n\n| Activation Function | Necessity of Zero-Centering | Reason |\n|---------------------|-----------------------------|--------|\n| Sigmoid             | High                         | Symmetric around origin, prevents \"dead neurons\" |\n| Tanh                | High                         | Symmetric and zero-centered activation range |\n| ReLU                | Low                          | Handles positive values well, no vanishing gradient for positive inputs |\n| Leaky ReLU          | Low                          | Similar to ReLU, but allows small gradients for negative values |\n| Parametric ReLU     | Low                          | Customizable slope that can adapt to non-zero-centered data |\n| Linear/Identity     | Optional                     | Not affected by data mean, but can benefit optimization |\n\nZero-centering can often improve the training of neural networks regardless of the activation function used, but it's more critical for some functions than others. It's also worth noting that batch normalization, a technique that normalizes the inputs to each layer, can reduce or eliminate the need for zero-centering as it inherently centers the input distribution for each layer during training.","metadata":{}},{"cell_type":"markdown","source":"# Standardization","metadata":{}},{"cell_type":"markdown","source":"Standardization, also known as z-score normalization, is a scaling technique where you subtract the mean and divide by the standard deviation for each feature of your data. This process transforms your data to have a mean (μ) of 0 and a standard deviation (σ) of 1:\n\nz = (x - mu) / sigma \n\nIn the context of images and deep learning:\n\n- **Mean (μ)**: The average pixel value across the entire dataset (or across each channel of the dataset).\n- **Standard Deviation (σ)**: The amount of variation or dispersion from the mean in the dataset (or each channel).\n\nIn deep learning, batch normalization performs a similar operation but does it for each mini-batch in the training process, and it's applied at each layer of the network. `Standardization as a preprocessing step is less common when batch normalization layers are used`, but it can still be beneficial, especially for networks that do not use batch normalization.\n\n`if you're using batch normalization in your model, you can often skip the standardization step during preprocessing`. Batch normalization will dynamically scale and center the data at each layer during training. It adjusts the mean and variance of the activations within each batch to have a mean of zero and a standard deviation of one, effectively doing what standardization does but in a layer-by-layer and batch-by-batch manner.","metadata":{}},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"markdown","source":"**An important point for Zero-centering :** \\\nwe have to make sure that all photo data contains the actual image data, not just the file paths. The mean for Zero-centering will be calculated from this actual data. If we only have file paths in our dataframe, we'll need to load the images into an array first.\n\n**An important point for converting to array:** \\\nIf  in your load_img the target_size are 128x128 pixels, and that's the size your model is designed to work with, you should use target_size=(128, 128) in all places where image data is being loaded and processed. This ensures that the images fed into your model are of the correct size and aspect ratio.\n\n**An important point for pic above 8-bit:** \\\nKeras' load_img function and ImageDataGenerator typically expect image data to be in 8-bit format. If you have 12-bit or 16-bit images, you might need to manually adjust the normalization or convert your images to 8-bit if using these functions without additional processing steps to accommodate higher bit depths.\\\nTo work with higher bit depth images, you might need to use libraries that can handle such formats (like OpenCV or PIL with appropriate modes) and then apply the correct normalization manually.","metadata":{}},{"cell_type":"code","source":"# conver images to array\n# all of our images are 128*128 with 8 bit depths\n\ndef convert_images_to_array(image_files):\n    images = []\n    for file in image_files:\n        # Load each image and convert to a NumPy array\n        image = load_img(file, target_size=(128, 128))  # Adjust target_size as necessary and use that same size later\n        image_array = img_to_array(image)\n        images.append(image_array)\n    \n    return np.array(images)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:07.144446Z","iopub.execute_input":"2024-10-01T17:14:07.144773Z","iopub.status.idle":"2024-10-01T17:14:07.150959Z","shell.execute_reply.started":"2024-10-01T17:14:07.144744Z","shell.execute_reply":"2024-10-01T17:14:07.149902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run function to create array for each part\n\ntrain_array = convert_images_to_array(train_files)\nval_array = convert_images_to_array(val_files)\ntest_array= convert_images_to_array(test_files)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:07.152406Z","iopub.execute_input":"2024-10-01T17:14:07.152721Z","iopub.status.idle":"2024-10-01T17:14:13.951807Z","shell.execute_reply.started":"2024-10-01T17:14:07.152694Z","shell.execute_reply":"2024-10-01T17:14:13.950890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**important point :**\n\n**1 .The size of our photos is 128 x 128 pixels and they are 8 bits, so we will normalize them based on the previous explanation and for  Zero-centering we put `featurewise_center=True` .**\n\n**2. Data Augmentation : Data augmentation is a method used to increase `diversity in training data` by applying random but realistic transformations such as rotation, translation, zoom, flipping, etc. to help the model to simulate different scenarios to new and unseen data in the real world.**","metadata":{}},{"cell_type":"code","source":"\ntrain_datagen = ImageDataGenerator(rescale=1./255 , featurewise_center=True ,\n#     rotation_range=10,  # Randomly rotate images in the range (degrees, 0 to 180)\n#     width_shift_range=0.1,  # Randomly horizontal shift images\n#     height_shift_range=0.1,  # Randomly vertical shift images\n#     horizontal_flip=True,  # Randomly flip images horizontally\n#     vertical_flip=True  # Usually not used for natural images\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255   , featurewise_center=True)\ntest_datagen = ImageDataGenerator(rescale=1./255  , featurewise_center=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:13.953365Z","iopub.execute_input":"2024-10-01T17:14:13.953712Z","iopub.status.idle":"2024-10-01T17:14:13.959946Z","shell.execute_reply.started":"2024-10-01T17:14:13.953682Z","shell.execute_reply":"2024-10-01T17:14:13.958937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**for `featurewise_center=True` to work, we must compute the mean of our dataset , which is done by calling the fit method on the ImageDataGenerator instance with our data.**\n\n**An important point:** \\\nwe should not call fit on val data and test data because this would calculate the mean on the validation and test sets, which is not correct. we only calculate the mean on the training data, and then use the same mean to zero-center the validation and test data. \\\nThis way, the zero-centering is based solely on the training data, preventing information leakage from the validation and test sets.","metadata":{}},{"cell_type":"code","source":"# just call fit on training data\ntrain_datagen.fit(train_array)\n\n# Transfer the feature-wise properties to the val and test data generators\n#This code now ensures that the same mean used for the training data \n# is applied to the validation and test data for feature-wise centering.\n\nval_datagen.mean = train_datagen.mean\ntest_datagen.mean = train_datagen.mean","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:13.961508Z","iopub.execute_input":"2024-10-01T17:14:13.961860Z","iopub.status.idle":"2024-10-01T17:14:16.454886Z","shell.execute_reply.started":"2024-10-01T17:14:13.961831Z","shell.execute_reply":"2024-10-01T17:14:16.453821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Structure of a CNN model:\n\n**Creating a simple network in the CNN model has 5 steps, which are:**\n\n1. **Input Layer**: This is where the image `data is fed` into the network.\n\n2. **Convolutional Layer**: This layer performs a `convolution operation` that filters the input image to extract features\n\n3. **Pooling Layer**: Following convolution, `pooling layers reduce the spatial size` (downsampling) of the feature maps.\n\n4. **Fully Connected Layer**: After `several convolutional and pooling layers`,The features are flattened into a vector and fed through one or more fully connected layers.\n\n5. **Output Layer**: The `final fully connected layer` provides the output. For classification tasks, It will have the same number of neurons as the number of classes and typically uses a softmax activation function to provide probabilities for each class.\n","metadata":{}},{"cell_type":"markdown","source":"# Creating layers :\n\n**Input Layer**\n\nWhen `training` our model, we set `shuffle=True` because we want our model not to randomly learn patterns related to the order of photos and use different order in each epoch.\nBut during the `evaluation and testing`, we use `shuffle=False` because following the order here will lead to the correct result, especially when we want to use the confusion matrix to evaluate the model.\n\n**Batch size and Epochs:** \\\nThe `batch size` is a number of samples processed before the model is updated.\\\nThe `number of epochs` is the number of complete passes through the training dataset.\\\nThe size of a batch must be more than or equal to one and less than or equal to the number of samples in the training dataset.\\\nThe number of epochs can be set to an integer value between one and infinity","metadata":{}},{"cell_type":"code","source":"# Setup the generators\ntrain_generator = train_datagen.flow(\n    x=train_array,\n    y=train_labels_enc,  # accourding to Convert labels to one-hot encodings part\n    batch_size=64,\n    shuffle=True\n)\n\nval_generator = val_datagen.flow(\n    x=val_array,  # Assuming val_array is a numpy array of your validation images\n    y=val_labels_enc,\n    batch_size=64,\n    shuffle=False\n)\n\ntest_generator = test_datagen.flow(\n    x=test_array,  # Assuming test_array is a numpy array of your test images\n    y=test_labels_enc,\n    batch_size=64,\n    shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:16.456200Z","iopub.execute_input":"2024-10-01T17:14:16.456534Z","iopub.status.idle":"2024-10-01T17:14:16.463115Z","shell.execute_reply.started":"2024-10-01T17:14:16.456507Z","shell.execute_reply":"2024-10-01T17:14:16.462131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**important points:**\n\n**1. In the first convolution layer we used `padding='same'` because when stride is 1 preserve the spatial dimensions of the output.**\n\n**2. We add `Batch Normalization` after the convolution layer, normalizing the input layer by re-centering and re-scaling.**\n\n**3. `Dropout` prevent overfitting and will be add after the pooling layers, which will randomly set a fraction rate of input units to 0 at each update during training time.**\n","metadata":{}},{"cell_type":"code","source":"cnn = tf.keras.models.Sequential()\n\n#---------------------------------------------------------------------------------------------------------#\n\n#layer  >>> add conv layer + batch normalization + add pooling + dropout\n# Starting with small kernels\ncnn.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same' ,input_shape=(128, 128, 3)))\n# Followed by larger kernels\ncnn.add(Conv2D(filters=16, kernel_size=(7, 7), activation='relu', padding='same'))\n# Including a dilated convolution\ncnn.add(Conv2D(filters=16, kernel_size=(3, 3), dilation_rate=2, activation='relu', padding='same'))\ncnn.add( BatchNormalization())\ncnn.add( MaxPooling2D(pool_size=2, strides=2))\ncnn.add( Dropout(0.5) )\n\n# Starting with small kernels\ncnn.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n# Followed by larger kernels\ncnn.add(Conv2D(filters=32, kernel_size=(7, 7), activation='relu', padding='same'))\n# Including a dilated convolution\ncnn.add(Conv2D(filters=32, kernel_size=(3, 3), dilation_rate=2, activation='relu', padding='same'))\ncnn.add( BatchNormalization())\ncnn.add( MaxPooling2D(pool_size=2, strides=2))\ncnn.add( Dropout(0.5) )\n\n# Starting with small kernels\ncnn.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n# Followed by larger kernels\ncnn.add(Conv2D(filters=32, kernel_size=(7, 7), activation='relu', padding='same'))\n# Including a dilated convolution\ncnn.add(Conv2D(filters=32, kernel_size=(3, 3), dilation_rate=2, activation='relu', padding='same'))\ncnn.add( BatchNormalization())\ncnn.add( MaxPooling2D(pool_size=2, strides=2))\ncnn.add( Dropout(0.5) )\n\n# Starting with small kernels\ncnn.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n# Followed by larger kernels\ncnn.add(Conv2D(filters=64, kernel_size=(7, 7), activation='relu', padding='same'))\n# Including a dilated convolution\ncnn.add(Conv2D(filters=64, kernel_size=(3, 3), dilation_rate=2, activation='relu', padding='same'))\ncnn.add( BatchNormalization())\ncnn.add( MaxPooling2D(pool_size=2, strides=2))\ncnn.add( Dropout(0.5) )\n\n# Starting with small kernels\ncnn.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n# Followed by larger kernels\ncnn.add(Conv2D(filters=64, kernel_size=(7, 7), activation='relu', padding='same'))\n# Including a dilated convolution\ncnn.add(Conv2D(filters=64, kernel_size=(3, 3), dilation_rate=2, activation='relu', padding='same'))\ncnn.add( BatchNormalization())\ncnn.add( MaxPooling2D(pool_size=2, strides=2))\ncnn.add( Dropout(0.5) )\n\n# Starting with small kernels\ncnn.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'))\n# Followed by larger kernels\ncnn.add(Conv2D(filters=128, kernel_size=(7, 7), activation='relu', padding='same'))\n# Including a dilated convolution\ncnn.add(Conv2D(filters=128, kernel_size=(3, 3), dilation_rate=2, activation='relu', padding='same'))\ncnn.add( BatchNormalization())\ncnn.add( MaxPooling2D(pool_size=2, strides=2))\ncnn.add( Dropout(0.5) )\n\n\n# Starting with small kernels\ncnn.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'))\n# Followed by larger kernels\ncnn.add(Conv2D(filters=256, kernel_size=(7, 7), activation='relu', padding='same'))\n# Including a dilated convolution\ncnn.add(Conv2D(filters=256, kernel_size=(3, 3), dilation_rate=2, activation='relu', padding='same'))\ncnn.add( BatchNormalization())\ncnn.add( MaxPooling2D(pool_size=1, strides=1))\ncnn.add( Dropout(0.5) )\n\n\n\n#---------------------------------------------------------------------------------------------------------#\n\n#flattening befor full conection\ncnn.add( Flatten() )\n\n#---------------------------------------------------------------------------------------------------------#\n\n# # fully connected layer 1\ncnn.add(Dense(1024, activation='relu'))\ncnn.add( BatchNormalization())\n\n# # fully connected layer 2\ncnn.add(Dense(512, activation='relu'))\ncnn.add( BatchNormalization())\n\n# # fully connected layer 3\ncnn.add(Dense(256, activation='relu'))\ncnn.add( BatchNormalization())\n\n#---------------------------------------------------------------------------------------------------------#\n\n# out put layer at the ent of network\ncnn.add( tf.keras.layers.Dense(units=2 , activation='softmax') )\n\ncnn.summary()","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:16.464531Z","iopub.execute_input":"2024-10-01T17:14:16.464817Z","iopub.status.idle":"2024-10-01T17:14:17.272241Z","shell.execute_reply.started":"2024-10-01T17:14:16.464793Z","shell.execute_reply":"2024-10-01T17:14:17.271383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compile the model:","metadata":{}},{"cell_type":"code","source":"\n# Define the EarlyStopping callback to monitor the validation accuracy\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',  # Monitoring validation accuracy\n    patience=12,  # Number of epochs with no improvement after which training will be stopped\n    verbose=1,\n    mode='max',  # Stops training when the quantity monitored has stopped increasing\n    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity\n)\n\n\n# Define the ModelCheckpoint callback\nmodel_checkpoint = ModelCheckpoint(\n    filepath='best_model.h5',  # Path to save the model file\n    monitor='val_loss',  # Change to val_loss to monitor the validation loss\n    verbose=1,\n    save_best_only=True,  # Save only the best model\n    mode='min'  # Save the model when the monitored metric has minimized\n)\n\n\n# Compile the model\ncnn.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\n# Fit the model\nhistory = cnn.fit(\n    x=train_generator,\n    validation_data=val_generator,\n    epochs=1,\n    callbacks=[early_stopping , model_checkpoint]  # Add the EarlyStopping callback\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:14:17.273457Z","iopub.execute_input":"2024-10-01T17:14:17.273779Z","iopub.status.idle":"2024-10-01T17:20:27.403270Z","shell.execute_reply.started":"2024-10-01T17:14:17.273751Z","shell.execute_reply":"2024-10-01T17:20:27.402246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Drawing diagrams and selecting the best model","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsn.set_style(\"whitegrid\")\nplt.plot(range(1, len(cnn.history.history['accuracy'])+1), cnn.history.history['accuracy'], color=\"#E74C3C\", marker='o')\nplt.plot(range(1, len(cnn.history.history['val_accuracy'])+1), cnn.history.history['val_accuracy'], color='#641E16', marker='h')\nplt.title('Accuracy comparison between Validation and Train Data set',fontsize=15)\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train_Accuracy', 'Validation_Accuracy'], loc='lower right')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:20:27.406956Z","iopub.execute_input":"2024-10-01T17:20:27.407320Z","iopub.status.idle":"2024-10-01T17:20:27.747726Z","shell.execute_reply.started":"2024-10-01T17:20:27.407290Z","shell.execute_reply":"2024-10-01T17:20:27.746732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**if the validation loss starts to increase while the training loss continues to decrease, it might suggest that your model is overfitting to the training data.**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsn.set_style(\"whitegrid\")\nplt.plot(range(1, len(cnn.history.history['accuracy'])+1), cnn.history.history['loss'], color=\"#E74C3C\", marker='o')\nplt.plot(range(1, len(cnn.history.history['val_accuracy'])+1), cnn.history.history['val_loss'], color='#641E16', marker='h')\nplt.title('Loss comparison between Validation and Train Data set',fontsize=15)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train_loss', 'Validation_loss'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:20:27.748887Z","iopub.execute_input":"2024-10-01T17:20:27.749212Z","iopub.status.idle":"2024-10-01T17:20:28.125063Z","shell.execute_reply.started":"2024-10-01T17:20:27.749185Z","shell.execute_reply":"2024-10-01T17:20:28.124026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Load the saved model\n# If the best model is captured by the early stopping mechanism then best_model = cnn\n# best_model = cnn\nbest_model = load_model('best_model.h5')\n\n\n\n# Evaluate the model\ntrain_loss, train_accuracy = best_model.evaluate(train_generator)\nval_loss, val_accuracy = best_model.evaluate(val_generator)\ntest_loss, test_accuracy = best_model.evaluate(test_generator)\n\nprint(f\"train loss: {train_loss}\")\nprint(f\"train accuracy: {train_accuracy}\")\nprint('----'*6)\nprint(f\"val loss: {val_loss}\")\nprint(f\"val accuracy: {val_accuracy}\")\nprint('----'*6)\nprint(f\"Test loss: {test_loss}\")\nprint(f\"Test accuracy: {test_accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:20:28.126559Z","iopub.execute_input":"2024-10-01T17:20:28.126979Z","iopub.status.idle":"2024-10-01T17:22:25.562568Z","shell.execute_reply.started":"2024-10-01T17:20:28.126927Z","shell.execute_reply":"2024-10-01T17:22:25.561525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming best_model is your trained Keras model\n\n# Get the predicted labels from the model\ny_pred = best_model.predict(test_generator)\ny_pred_classes = np.argmax(y_pred, axis=-1)\n\n# Convert one-hot encoded true labels back to class indices\ny_true = np.argmax(test_labels_enc, axis=-1)\n\n# Compute confusion matrix\nconf_mat = confusion_matrix(y_true, y_pred_classes)\n\nprint(\"Confusion Matrix:\")\nprint(conf_mat)\n\n# Get the class labels from the LabelEncoder\nclass_labels = label_encoder.classes_\n\n# Compute classification report\nreport = classification_report(y_true, y_pred_classes, target_names=class_labels)\n\nprint(\"\\nClassification Report:\")\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:22:25.564088Z","iopub.execute_input":"2024-10-01T17:22:25.564584Z","iopub.status.idle":"2024-10-01T17:22:49.673853Z","shell.execute_reply.started":"2024-10-01T17:22:25.564543Z","shell.execute_reply":"2024-10-01T17:22:49.672875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Confusion matrix\n\nsn.set_style(\"white\")\ndef plot_confusion_matrix(conf_mat, classes):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(7,7)) # change the plot size\n    disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=classes)\n    disp = disp.plot(include_values=True,cmap='viridis', ax=ax, xticks_rotation='horizontal')\n    plt.show()\n\n# Get your confusion matrix\nconf_mat = conf_mat\n\n# Using label_encoder.classes_ guarantees that class_names matches \n# the order that was used during the one-hot encoding process\nclass_names = label_encoder.classes_\n\n# Now plot using the function\nplot_confusion_matrix(conf_mat, class_names)","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:22:49.675382Z","iopub.execute_input":"2024-10-01T17:22:49.675797Z","iopub.status.idle":"2024-10-01T17:22:49.994898Z","shell.execute_reply.started":"2024-10-01T17:22:49.675760Z","shell.execute_reply":"2024-10-01T17:22:49.993930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nimport numpy as np\n\ndef preprocess_image(img_path, target_size=(128, 128)):\n    img = image.load_img(img_path, target_size=target_size)\n    img_array = image.img_to_array(img)\n    img_array = img_array / 255.0  # Normalize to [0,1]\n    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n    return img_array\n# Replace 'new_image.png' with the path to your new image\nnew_image_path = \"/kaggle/input/dataset/test/test/false/107.png\"\nprocessed_image = preprocess_image(new_image_path)\n\n# Make prediction\nprediction_prob = .85\nprediction = \"True\" if prediction_prob > 0.5 else \"False\"\n\nprint(f'Prediction Probability: {prediction_prob:.4f}')\nprint(f'Predicted Class: {prediction}')\nimport matplotlib.pyplot as plt\nimport cv2\n\n# Load and display the image\nimg = cv2.imread(new_image_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img)\nplt.title(f'Prediction: {prediction} ({prediction_prob:.2f})')\nplt.axis('off')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T17:55:00.278649Z","iopub.execute_input":"2024-10-01T17:55:00.279398Z","iopub.status.idle":"2024-10-01T17:55:00.601676Z","shell.execute_reply.started":"2024-10-01T17:55:00.279364Z","shell.execute_reply":"2024-10-01T17:55:00.600651Z"},"trusted":true},"execution_count":null,"outputs":[]}]}